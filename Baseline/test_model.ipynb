{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "r\"\"\"Binary for trianing a RNN-based classifier for the Quick, Draw! data.\n",
    "\n",
    "python train_model.py \\\n",
    "  --training_data train_data \\\n",
    "  --eval_data eval_data \\\n",
    "  --model_dir /tmp/quickdraw_model/ \\\n",
    "  --cell_type cudnn_lstm\n",
    "\n",
    "When running on GPUs using --cell_type cudnn_lstm is much faster.\n",
    "\n",
    "The expected performance is ~75% in 1.5M steps with the default configuration.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import ast\n",
    "import functools\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_num_classes():\n",
    "  classes = []\n",
    "  with tf.gfile.GFile(FLAGS.classes_file, \"r\") as f:\n",
    "    classes = [x for x in f]\n",
    "  num_classes = len(classes)\n",
    "  return num_classes\n",
    "\n",
    "\n",
    "def get_input_fn(mode, tfrecord_pattern, batch_size):\n",
    "  \"\"\"Creates an input_fn that stores all the data in memory.\n",
    "\n",
    "  Args:\n",
    "   mode: one of tf.contrib.learn.ModeKeys.{TRAIN, INFER, EVAL}\n",
    "   tfrecord_pattern: path to a TF record file created using create_dataset.py.\n",
    "   batch_size: the batch size to output.\n",
    "\n",
    "  Returns:\n",
    "    A valid input_fn for the model estimator.\n",
    "  \"\"\"\n",
    "\n",
    "  def _parse_tfexample_fn(example_proto, mode):\n",
    "    \"\"\"Parse a single record which is expected to be a tensorflow.Example.\"\"\"\n",
    "    feature_to_type = {\n",
    "        \"ink\": tf.VarLenFeature(dtype=tf.float32),\n",
    "        \"shape\": tf.FixedLenFeature([2], dtype=tf.int64)\n",
    "    }\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "      # The labels won't be available at inference time, so don't add them\n",
    "      # to the list of feature_columns to be read.\n",
    "      feature_to_type[\"class_index\"] = tf.FixedLenFeature([1], dtype=tf.int64)\n",
    "\n",
    "    parsed_features = tf.parse_single_example(example_proto, feature_to_type)\n",
    "    labels = None\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "      labels = parsed_features[\"class_index\"]\n",
    "    parsed_features[\"ink\"] = tf.sparse_tensor_to_dense(parsed_features[\"ink\"])\n",
    "    return parsed_features, labels\n",
    "\n",
    "  def _input_fn():\n",
    "    \"\"\"Estimator `input_fn`.\n",
    "\n",
    "    Returns:\n",
    "      A tuple of:\n",
    "      - Dictionary of string feature name to `Tensor`.\n",
    "      - `Tensor` of target labels.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.TFRecordDataset.list_files(tfrecord_pattern)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      dataset = dataset.shuffle(buffer_size=10)\n",
    "    dataset = dataset.repeat()\n",
    "    # Preprocesses 10 files concurrently and interleaves records from each file.\n",
    "    dataset = dataset.interleave(\n",
    "        tf.data.TFRecordDataset,\n",
    "        cycle_length=10,\n",
    "        block_length=1)\n",
    "    dataset = dataset.map(\n",
    "        functools.partial(_parse_tfexample_fn, mode=mode),\n",
    "        num_parallel_calls=10)\n",
    "    dataset = dataset.prefetch(10000)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      dataset = dataset.shuffle(buffer_size=1000000)\n",
    "    # Our inputs are variable length, so pad them.\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size, padded_shapes=dataset.output_shapes)\n",
    "    features, labels = dataset.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n",
    "\n",
    "  return _input_fn\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "  \"\"\"Model function for RNN classifier.\n",
    "\n",
    "  This function sets up a neural network which applies convolutional layers (as\n",
    "  configured with params.num_conv and params.conv_len) to the input.\n",
    "  The output of the convolutional layers is given to LSTM layers (as configured\n",
    "  with params.num_layers and params.num_nodes).\n",
    "  The final state of the all LSTM layers are concatenated and fed to a fully\n",
    "  connected layer to obtain the final classification scores.\n",
    "\n",
    "  Args:\n",
    "    features: dictionary with keys: inks, lengths.\n",
    "    labels: one hot encoded classes\n",
    "    mode: one of tf.estimator.ModeKeys.{TRAIN, INFER, EVAL}\n",
    "    params: a parameter dictionary with the following keys: num_layers,\n",
    "      num_nodes, batch_size, num_conv, conv_len, num_classes, learning_rate.\n",
    "\n",
    "  Returns:\n",
    "    ModelFnOps for Estimator API.\n",
    "  \"\"\"\n",
    "\n",
    "  def _get_input_tensors(features, labels):\n",
    "    \"\"\"Converts the input dict into inks, lengths, and labels tensors.\"\"\"\n",
    "    # features[ink] is a sparse tensor that is [8, batch_maxlen, 3]\n",
    "    # inks will be a dense tensor of [8, maxlen, 3]\n",
    "    # shapes is [batchsize, 2]\n",
    "    shapes = features[\"shape\"]\n",
    "    # lengths will be [batch_size]\n",
    "    lengths = tf.squeeze(\n",
    "        tf.slice(shapes, begin=[0, 0], size=[params.batch_size, 1]))\n",
    "    inks = tf.reshape(features[\"ink\"], [params.batch_size, -1, 3])\n",
    "    if labels is not None:\n",
    "      labels = tf.squeeze(labels)\n",
    "    return inks, lengths, labels\n",
    "\n",
    "  def _add_conv_layers(inks, lengths):\n",
    "    \"\"\"Adds convolution layers.\"\"\"\n",
    "    convolved = inks\n",
    "    for i in range(len(params.num_conv)):\n",
    "      convolved_input = convolved\n",
    "      if params.batch_norm:\n",
    "        convolved_input = tf.layers.batch_normalization(\n",
    "            convolved_input,\n",
    "            training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "      # Add dropout layer if enabled and not first convolution layer.\n",
    "      if i > 0 and params.dropout:\n",
    "        convolved_input = tf.layers.dropout(\n",
    "            convolved_input,\n",
    "            rate=params.dropout,\n",
    "            training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "      convolved = tf.layers.conv1d(\n",
    "          convolved_input,\n",
    "          filters=params.num_conv[i],\n",
    "          kernel_size=params.conv_len[i],\n",
    "          activation=None,\n",
    "          strides=1,\n",
    "          padding=\"same\",\n",
    "          name=\"conv1d_%d\" % i)\n",
    "    return convolved, lengths\n",
    "\n",
    "  def _add_regular_rnn_layers(convolved, lengths):\n",
    "    \"\"\"Adds RNN layers.\"\"\"\n",
    "    if params.cell_type == \"lstm\":\n",
    "      cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "    elif params.cell_type == \"block_lstm\":\n",
    "      cell = tf.contrib.rnn.LSTMBlockCell\n",
    "    cells_fw = [cell(params.num_nodes) for _ in range(params.num_layers)]\n",
    "    cells_bw = [cell(params.num_nodes) for _ in range(params.num_layers)]\n",
    "    if params.dropout > 0.0:\n",
    "      cells_fw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_fw]\n",
    "      cells_bw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_bw]\n",
    "    outputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw=cells_fw,\n",
    "        cells_bw=cells_bw,\n",
    "        inputs=convolved,\n",
    "        sequence_length=lengths,\n",
    "        dtype=tf.float32,\n",
    "        scope=\"rnn_classification\")\n",
    "    return outputs\n",
    "\n",
    "  def _add_cudnn_rnn_layers(convolved):\n",
    "    \"\"\"Adds CUDNN LSTM layers.\"\"\"\n",
    "    # Convolutions output [B, L, Ch], while CudnnLSTM is time-major.\n",
    "    convolved = tf.transpose(convolved, [1, 0, 2])\n",
    "    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\n",
    "        num_layers=params.num_layers,\n",
    "        num_units=params.num_nodes,\n",
    "        dropout=params.dropout if mode == tf.estimator.ModeKeys.TRAIN else 0.0,\n",
    "        direction=\"bidirectional\")\n",
    "    outputs, _ = lstm(convolved)\n",
    "    # Convert back from time-major outputs to batch-major outputs.\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    return outputs\n",
    "\n",
    "  def _add_rnn_layers(convolved, lengths):\n",
    "    \"\"\"Adds recurrent neural network layers depending on the cell type.\"\"\"\n",
    "    if params.cell_type != \"cudnn_lstm\":\n",
    "      outputs = _add_regular_rnn_layers(convolved, lengths)\n",
    "    else:\n",
    "      outputs = _add_cudnn_rnn_layers(convolved)\n",
    "    # outputs is [batch_size, L, N] where L is the maximal sequence length and N\n",
    "    # the number of nodes in the last layer.\n",
    "    mask = tf.tile(\n",
    "        tf.expand_dims(tf.sequence_mask(lengths, tf.shape(outputs)[1]), 2),\n",
    "        [1, 1, tf.shape(outputs)[2]])\n",
    "    zero_outside = tf.where(mask, outputs, tf.zeros_like(outputs))\n",
    "    outputs = tf.reduce_sum(zero_outside, axis=1)\n",
    "    return outputs\n",
    "\n",
    "  def _add_fc_layers(final_state):\n",
    "    \"\"\"Adds a fully connected layer.\"\"\"\n",
    "    return tf.layers.dense(final_state, params.num_classes)\n",
    "\n",
    "  # Build the model.\n",
    "  inks, lengths, labels = _get_input_tensors(features, labels)\n",
    "  convolved, lengths = _add_conv_layers(inks, lengths)\n",
    "  final_state = _add_rnn_layers(convolved, lengths)\n",
    "  logits = _add_fc_layers(final_state)\n",
    "  # Add the loss.\n",
    "  cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          labels=labels, logits=logits))\n",
    "  # Add the optimizer.\n",
    "  train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss=cross_entropy,\n",
    "      global_step=tf.train.get_global_step(),\n",
    "      learning_rate=params.learning_rate,\n",
    "      optimizer=\"Adam\",\n",
    "      # some gradient clipping stabilizes training in the beginning.\n",
    "      clip_gradients=params.gradient_clipping_norm,\n",
    "      summaries=[\"learning_rate\", \"loss\", \"gradients\", \"gradient_norm\"])\n",
    "  # Compute current predictions.\n",
    "  predictions = tf.argmax(logits, axis=1)\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions={\"logits\": logits, \"predictions\": predictions},\n",
    "      loss=cross_entropy,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={\"accuracy\": tf.metrics.accuracy(labels, predictions)})\n",
    "\n",
    "\n",
    "def create_estimator_and_specs(run_config):\n",
    "  \"\"\"Creates an Experiment configuration based on the estimator and input fn.\"\"\"\n",
    "  model_params = tf.contrib.training.HParams(\n",
    "      num_layers=FLAGS.num_layers,\n",
    "      num_nodes=FLAGS.num_nodes,\n",
    "      batch_size=FLAGS.batch_size,\n",
    "      num_conv=ast.literal_eval(FLAGS.num_conv),\n",
    "      conv_len=ast.literal_eval(FLAGS.conv_len),\n",
    "      num_classes=get_num_classes(),\n",
    "      learning_rate=FLAGS.learning_rate,\n",
    "      gradient_clipping_norm=FLAGS.gradient_clipping_norm,\n",
    "      cell_type=FLAGS.cell_type,\n",
    "      batch_norm=FLAGS.batch_norm,\n",
    "      dropout=FLAGS.dropout)\n",
    "\n",
    "  estimator = tf.estimator.Estimator(\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      params=model_params)\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(input_fn=get_input_fn(\n",
    "      mode=tf.estimator.ModeKeys.TRAIN,\n",
    "      tfrecord_pattern=FLAGS.training_data,\n",
    "      batch_size=FLAGS.batch_size), max_steps=FLAGS.steps)\n",
    "\n",
    "  eval_spec = tf.estimator.EvalSpec(input_fn=get_input_fn(\n",
    "      mode=tf.estimator.ModeKeys.EVAL,\n",
    "      tfrecord_pattern=FLAGS.eval_data,\n",
    "      batch_size=FLAGS.batch_size))\n",
    "\n",
    "  return estimator, train_spec, eval_spec\n",
    "\n",
    "\n",
    "def main(unused_args):\n",
    "  estimator, train_spec, eval_spec = create_estimator_and_specs(\n",
    "      run_config=tf.estimator.RunConfig(\n",
    "          model_dir=FLAGS.model_dir,\n",
    "          save_checkpoints_secs=300,\n",
    "          save_summary_steps=100))\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "  parser.add_argument(\n",
    "      \"--training_data\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path to training data (tf.Example in TFRecord format)\")\n",
    "  parser.add_argument(\n",
    "      \"--eval_data\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path to evaluation data (tf.Example in TFRecord format)\")\n",
    "  parser.add_argument(\n",
    "      \"--classes_file\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path to a file with the classes - one class per line\")\n",
    "  parser.add_argument(\n",
    "      \"--num_layers\",\n",
    "      type=int,\n",
    "      default=3,\n",
    "      help=\"Number of recurrent neural network layers.\")\n",
    "  parser.add_argument(\n",
    "      \"--num_nodes\",\n",
    "      type=int,\n",
    "      default=128,\n",
    "      help=\"Number of node per recurrent network layer.\")\n",
    "  parser.add_argument(\n",
    "      \"--num_conv\",\n",
    "      type=str,\n",
    "      default=\"[48, 64, 96]\",\n",
    "      help=\"Number of conv layers along with number of filters per layer.\")\n",
    "  parser.add_argument(\n",
    "      \"--conv_len\",\n",
    "      type=str,\n",
    "      default=\"[5, 5, 3]\",\n",
    "      help=\"Length of the convolution filters.\")\n",
    "  parser.add_argument(\n",
    "      \"--cell_type\",\n",
    "      type=str,\n",
    "      default=\"lstm\",\n",
    "      help=\"Cell type used for rnn layers: cudnn_lstm, lstm or block_lstm.\")\n",
    "  parser.add_argument(\n",
    "      \"--batch_norm\",\n",
    "      type=\"bool\",\n",
    "      default=\"False\",\n",
    "      help=\"Whether to enable batch normalization or not.\")\n",
    "  parser.add_argument(\n",
    "      \"--learning_rate\",\n",
    "      type=float,\n",
    "      default=0.0001,\n",
    "      help=\"Learning rate used for training.\")\n",
    "  parser.add_argument(\n",
    "      \"--gradient_clipping_norm\",\n",
    "      type=float,\n",
    "      default=9.0,\n",
    "      help=\"Gradient clipping norm used during training.\")\n",
    "  parser.add_argument(\n",
    "      \"--dropout\",\n",
    "      type=float,\n",
    "      default=0.3,\n",
    "      help=\"Dropout used for convolutions and bidi lstm layers.\")\n",
    "  parser.add_argument(\n",
    "      \"--steps\",\n",
    "      type=int,\n",
    "      default=100000,\n",
    "      help=\"Number of training steps.\")\n",
    "  parser.add_argument(\n",
    "      \"--batch_size\",\n",
    "      type=int,\n",
    "      default=8,\n",
    "      help=\"Batch size to use for training/evaluation.\")\n",
    "  parser.add_argument(\n",
    "      \"--model_dir\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path for storing the model checkpoints.\")\n",
    "  parser.add_argument(\n",
    "      \"--self_test\",\n",
    "      type=\"bool\",\n",
    "      default=\"False\",\n",
    "      help=\"Whether to enable batch normalization or not.\")\n",
    "\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
